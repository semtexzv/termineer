## $ support
- We should be able to run shell commands directly from the user interface.

## Single query mode.
- We used to have a feature where you could provide a query as a last argument and the application would run output the agent output and then finish whenever the agent was done.
- reimplement this feature where we do not start the terminal user interface. We just accept one query as a command line argument and then we run the single agent until it goes into done state and then we output it's last message
- We should also output its intermediate output, like the buffer, into standard error.
- We should also support interruptions in this mode.

## Agent tool
- Give running agents the ability to spawn new agents. Also give them ability to send messages to other agents.
- Messages are delivered in the same way as user messages are. However, when they are injected into the agent conversation, they are injected with a XML tags, similar to how we use XML everywhere else. And the originating agent name is provided in the XML tags.
- This all should be implemented under the agent tool that will have subcommands. First subcommand is create where the tool body will be the query used for the subagent. Second subcommand should be send which will send a message to another agent.
- There should be a new wait command that the agent can use to wait for messages from other agents.The implementation should be similar to the done command. Basically the agent is put into the non-running state and any message from either user or other agents will put it into the running state.
- This nonrunning state should possibly be the existing idle state.

# LLM backend timeout and retry.
- We should run the backend API calls with a really long timeout, let's say 30 to 60 seconds.
- For example, when we receive a network issue, we should utilize a linear back-off with a maximum waiting time of 30 seconds.

# Agent shell interruption duration info.
- Whenever we are running a shell, the agent doesn't know for how long the command has been running when we ask it for interruption. Let's include this info in that small query that we appended to the conversation in the interruption check.

