
# LLM backend timeout and retry.
- We should run the backend API calls with a really long timeout, let's say 100 to 200 seconds.
- For example, when we receive a network issue, we should utilize a linear back-off with a maximum waiting time of 30 seconds.
- Maybe use a library timeout to detect network issues.

# Simple Sessions.
- Whenever user is working in the directory save the session periodically or after every model output to a file
- This file should be called .session.

# Bugs:
- Wrapping logic is not correct in the input box, multiline text the cursor is one character behind per line
- Tools template needs to specify that write tool will overwrite entire file contents

# Modify truncation
We're not saving enough tokens by simple tool output truncation, truncate tool invocations as well, replace them by simple
assistant message that says what was performed (Eg, I edited this file. I printend this file contents).

# Asynchronous shell.
Allow running commands asynchronously, their new output being periodically injected into the conversation. Agent
should be able to start a shell, then would receive the output periodically, and then can kill the shell.
 Would allow us testing the application itself better.

# Read tool does not colorize output correctly
Make sure all lines are grayed, not just first.
